---
layout: page
title: "Multi Layer Perceptron Layers"
category: tut
date: 2015-04-29 12:00:00
---

__I assume that you have the knowledge and understand following knowledge__

+ __Multi Layer Perceptron__

+ __Activation__

+ __[Feedforward layer](tut-layer.html)__

### Activation Function

In this section we introduce 6 popular activation function for the neuron. They are usually introduced in Deep Neural Network research.

As you can see, all of them are simple functions that can be called from library easily. So why would I write it anyway. I decided to create a mask so that I don't have to bother with remembering the current path from different places. The following is an example ([here](https://github.com/duguyue100/telaugesa/blob/master/telaugesa/nnfuns.py)):

~~~ python
import theano.tensor as T

def tanh(x):
    """Hyperbolic tangent nonlinearity"""
    return T.tanh(x);

def sigmoid(x):
    """Standard sigmoid nonlinearity"""
    return T.nnet.sigmoid(x);

def softplus(x):
    """Softplus nonlinearity"""
    return T.nnet.softplus(x);

def relu(x):
    """Rectified linear unit"""
    return x*(x>1e-13);

def softmax(x):
    """Softmax function"""
    return T.nnet.softmax(x);

def identity(x):
    """Identity function"""
    return x;
~~~

### Activate a Layer

Suppose a set of samples \\(X\\) and weights \\(W\\), the bias \\(b\\), then the activation can be computed as:

$$A=f(Y)$$

where \\(Y=W\cdot X+b\\). Turns out, this is extremely easy to write in Python.

In python, you can write it as ([example](https://github.com/duguyue100/telaugesa/blob/master/telaugesa/fflayers.py#L36-L37)):

~~~ python
return activation(self.apply_lin(X));
~~~

Note `apply_lin` is the linear transformation function from your abstract `Layer` class in [Feedforward Layer](tut-layer.html).

### MLP Layers

We mostly interested in 4 type of layers:

+ Identity layer: apply identity function as activation.

+ Hyperbolic tangent layer: apply \\(\tanh\\) function as activation

+ Sigmoid Layer: apply sigmoid function as activation

+ ReLU layer: apply rectified linear unit as activation

The above 4 usually served as hidden layer in MLP network. Softmax layer is usually used as classification layer, we introduced it in [Softmax Regression](tut-svm-softmax.html).

If you write your `Layer` correctly, then it's very easy to extend them to be the above mentioned 4 types of layers. Here is an example ([here](https://github.com/duguyue100/telaugesa/blob/master/telaugesa/fflayers.py#L17-L58)):

~~~ python
class IdentityLayer(Layer):
    """Identity Layer
    """
    
    def __init__(self, **kwargs):
        super(IdentityLayer, self).__init__(**kwargs);
    
    def apply(self, X):
        return self.apply_lin(X);
        
class TanhLayer(Layer):
    """Tanh Layer
    """
    
    def __init__(self, **kwargs):
        super(TanhLayer, self).__init__(**kwargs);
        
        self.initialize("tanh");
        
    def apply(self, X):
        return nnfuns.tanh(self.apply_lin(X));
    
class SigmoidLayer(Layer):
    """Sigmoid Layer"""
    
    def __init__(self, **kwargs):
        
        super(SigmoidLayer, self).__init__(**kwargs);
        
        self.initialize("sigmoid");
        
    def apply(self, X):
        return nnfuns.sigmoid(self.apply_lin(X));

class ReLULayer(Layer):
    """ReLU Layer"""
    
    def __init__(self, **kwargs):
        super(ReLULayer, self).__init__(**kwargs);
        
    def apply(self, X):
        return nnfuns.relu(self.apply_lin(X));
~~~

In above codes, `TanhLayer` and `SigmoidLayer` is a little different in `__init__` than others. They can a better weight initialization strategy.

### Initialize Weights

There are few heuristics that we can apply when we initialize the weights of a MLP layer. It should be uniformly sampled from a symmetric interval that depends on the activation function.  If the activation function is \\(\tanh\\), then the interval is

$$
\left[-\sqrt{\frac{6}{fan_{in}+fan_{out}}}, \sqrt{\frac{6}{fan_{in}+fan_{out}}}\right]
$$

where \\(fan_{in}\\) is number of neuron of \\((i-1)\\)-th layer and \\(fan_{out}\\) is the number of neuron of \\(i\\)-th layer. For sigmoid function, the range is

$$
\left[-4\sqrt{\frac{6}{fan_{in}+fan_{out}}}, 4\sqrt{\frac{6}{fan_{in}+fan_{out}}}\right]
$$

Generally, this boundary should be close to 0 and weights are randomly generated.

Related contribution can be found at this paper: [Understanding the difficulty of training deep feedforward neuralnetworks](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf).

The following is an example for a weight initialization function ([here](https://github.com/duguyue100/telaugesa/blob/master/telaugesa/util.py)):

~~~ python
def init_weights(name,
                 out_dim,
                 in_dim=None,
                 weight_type="none"):
    """Create shared weights or bias
    
    Parameters
    ----------
    out_dim : int
        output dimension
    in_dim : int
        input dimension
    weight_type : string
        type of weights: "none", "tanh", "sigmoid"
    
    Returns
    -------
    Weights : matrix or vector
        shared matrix with respect size
    """
  
    if in_dim is not None:
        if weight_type=="tanh":
            lower_bound=-np.sqrt(6. / (in_dim + out_dim));
            upper_bound=np.sqrt(6. / (in_dim + out_dim));
        elif weight_type=="sigmoid":
            lower_bound=-4*np.sqrt(6. / (in_dim + out_dim));
            upper_bound=4*np.sqrt(6. / (in_dim + out_dim));
        elif weight_type=="none":
            lower_bound=0;
            upper_bound=1./(in_dim+out_dim);
  
    if in_dim==None:
        return theano.shared(value=np.asarray(np.random.uniform(low=0,
                                                                high=1./out_dim,
                                                                size=(out_dim, )),
                                              dtype="float32"),
                             name=name,
                             borrow=True);
    else:
        return theano.shared(value=np.asarray(np.random.uniform(low=lower_bound,
                                                                high=upper_bound,
                                                                size=(in_dim, out_dim)),
                                              dtype="float32"),
                             name=name,
                             borrow=True);
~~~
