---
layout: page
title: "Feedforward Layer"
category: tut
date: 2015-06-11 12:00:00
---

__This tutorial assumes that you understand the basic knowledge of Feedforward Neural Networks, including concepts of__

+ __input neuron__
+ __output neuron__
+ __weight matrix__
+ __bias__

### Initialize an Abstract Layer

A _Feedforward Neural Network_ (FNN) consists of series of fully-connected layers. These layers has some common properties so that we can write an abstract layer so that we don't have to write them again and again for different types of feedforward layer.

A FNN layer is defined by at least 4 parameters:

+ input dimension

+ output dimension

+ Weight matrix

+ Bias vector

With these 4 parameters, we can generate or load a layer as wish.

The following is an example implementation of `__init__` method ([here](https://github.com/duguyue100/telaugesa/blob/master/telaugesa/layer.py#L18-L55)):

~~~ python
class Layer(object):
    """Abstract layer for Feed-forward Neural Networks"""
    
    def __init__(self,
                 in_dim,
                 out_dim,
                 layer_name="Layer",
                 W=None,
                 bias=None,
                 use_bias=True,
                 is_recursive=False,
                 **kwargs):
        """Base Layer initalization
        
        Parameters
        ----------
        in_dim : int
            input dimension of the layer
        out_dim : int
            output dimension of the layer
        W : matrix
            weight matrix for the layer, the size should be (in_dim, out_dim),
            if it is None, then the class will create one
        bias : vector
            bias vector for the layer, the size should be (out_dim),
            if it is None, then the class will create one
        """
        
        self.in_dim=in_dim;
        self.out_dim=out_dim;
        self.W=W;
        self.bias=bias;
        self.use_bias=use_bias;
        self.is_recursive=is_recursive;
        
        self.initialize();
        
        super(Layer, self).__init__(**kwargs);
~~~

Usually, we would like to initialize the weight when we create a layer. I used to initialize them in `__init__`, however, this could be messy when you try to extend this class to a more complex one. Therefore, I wrote an __initialize__ function to make initialization more flexible ([here](https://github.com/duguyue100/telaugesa/blob/master/telaugesa/layer.py#L57-L70)):

~~~ python
    def initialize(self, weight_type="none"):
        """Initialize weights and bias
        
        Parameters
        ----------
        weight_type : string
            type of weights: "none", "tanh", "sigmoid"
        """
        
        if self.W==None:
            self.W=util.init_weights("W", self.out_dim, self.in_dim, weight_type=weight_type);
            
        if self.use_bias==True and self.bias==None:
            self.bias=util.init_weights("bias", self.out_dim, weight_type=weight_type);
~~~

Since `numpy` recognize a 1-D array as row vector, therefore we prefer to use row based structure for our data. This means that each row is one piece of data. Hence, your data \\(X\\) is in size of `number of number * size of sample`. And the linear transformation between input \\(X\\), output \\(Y\\), weights \\(W\\) and bias \\(b\\) is

$$
Y=W\cdot X+b
$$

A example of writing it is ([here](https://github.com/duguyue100/telaugesa/blob/master/telaugesa/layer.py#L72-L91)):

~~~ python
    def apply_lin(self, X):
        """Apply linear transformation
        
        Parameters
        ----------
        X : matrix
            input samples, the size is (number of cases, in_dim)
            
        Returns
        -------
        Y : matrix
            output results, the size is (number of cases, out_dim);
        """
        
        Y=T.dot(X, self.W);
        
        if self.use_bias==True:
            Y+=self.bias;
        
        return Y;
~~~

In above code, I introduced a parameter `use_bias`. If you set this to `false`, then you will compute a set of linear equations that are not includes bias in Y-axis.

Surprisingly, there is not much work to write a general layer. The above 3 functions is actually enough to create a sufficient Feedforward Layer.
